{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c55bf5c",
   "metadata": {},
   "source": [
    "The NN should be able to predict hand written digits from $0$ to $9$, with more than $90\\%$ accuray. To train and test the accuracy of the NN, the MNIST data set is used. The training data consists of $60.000$ grayscaled images with a size of $28\\times 28$ pixels, with corresponding labels, and the testing data consists of $10.000$ grayscaled images of same size, with corresponding labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1b2db",
   "metadata": {},
   "source": [
    "The Neural Netowrk has the following architecture.\n",
    "- Input layer $(784\\times 1)$\n",
    "- Hidden layer $(100\\times 1)$\n",
    "- Output layer $(10\\times 1)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993dae3",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77fdfd",
   "metadata": {},
   "source": [
    "##### Input Layer\n",
    "To optimize the program, the images are converted from matrices of size $28\\times 28$ to vectors of size $784\\times 1$. This vector is used as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eacf91",
   "metadata": {},
   "source": [
    "##### Hidden layer\n",
    "The hidden layer is a fully connected layer with 100 units. Each unit calculates the weighted sum of the input and adds a bias number. \n",
    "$$\n",
    "z_{j} = \\sum_{i=0}^{m-1} (x_iw_{ij}) + b_j,\\; m=784\n",
    "$$\n",
    "Here $z_j$ is the weighted sum plus bias of the *j'th* unit in the hidden layer. $x_i$ is each pixel of the input data and $w_{ij}$ is the weight between the *i'th* pixel and the *j'th* unit. $b_j$ is the bias of the *j'th* unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3e4c6",
   "metadata": {},
   "source": [
    "In practice this is computed with matrix multiplication. To include the bias a row of 1 is added to the input so $X\\in\\mathbb{R^{785\\times 1}}$. Likewise, an extra column is initialized for the weight matrix $W_0\\in\\mathbb{R^{100\\times 785}}$.\\\n",
    "This enables,\n",
    "$$\n",
    "Z_0 = W_0X ,\\; Z_0\\in\\mathbb{R^{100\\times 1}} \\\\[5pt]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a180d9",
   "metadata": {},
   "source": [
    "\n",
    "Afterwards a non-linear activation function is used on the sum, in this case the activation function is the rectified linear unit function *(ReLU)*.\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases}\n",
    "x, & \\text{if } x > 0 \\\\\n",
    "0, & \\text{if } x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "a_j = \\text{ReLU}(z_j) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1debe06",
   "metadata": {},
   "source": [
    "This can also be used with matrices for quicker computations,\n",
    "$$\n",
    "A_0 = \\text{ReLU}(Z_0),\\; A_0\\in\\mathbb{R^{100\\times 1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c0871",
   "metadata": {},
   "source": [
    "During training, before the output layer, dropout is applied to the hidden layer activations $A_0$. For each entry in $A_0$ a random value is sampled in range $[0, 1]$. If the value is below the dropout rate (e.g. $40\\%$), the entry is set to $0$. This stochastic removal of units helps to prevent overfitting, by forcing the network to avoid over-reliance on specific untis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5af3c3",
   "metadata": {},
   "source": [
    "##### Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c9986",
   "metadata": {},
   "source": [
    "The output layer also calculates the weighted sum plus bias,\n",
    "$$\n",
    "Z_1 = W_1A_0\n",
    "$$\n",
    "with $W_1\\in\\mathbb{R^{10\\times 100}}$ resulting in the matrix $Z_1\\in\\mathbb{R^{10\\times 1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaaad7d",
   "metadata": {},
   "source": [
    "Afterwards, the softmax function is applied instead of ReLU, as the activation function.\n",
    "$$\n",
    "\\text{softmax}(Z) = \\frac{e^{z_i}}{\\sum_{j=0}^{10-1}e^{z_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051a9f3",
   "metadata": {},
   "source": [
    "This transforms the output vector into a probability distribution $\\hat{y}\\in\\mathbb{R^{10\\times 1}}$, where each unit $\\hat{y}_i\\in[0, 1]$ and,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53fb19",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=0}^{10-1}\\hat{y}_i=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56cee9",
   "metadata": {},
   "source": [
    "This is ideal for multi-class classifaction, because each output unit represents the probability of the given class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ca84e",
   "metadata": {},
   "source": [
    "The weight matrices $W_0$ and $W_1$ are initialized with He-initialization. This helps preventing vanishing or exploding gradients.\\\n",
    "This gives us,\n",
    "$$\n",
    "W_j \\rightarrow \\mathcal{N}(0, \\sqrt{\\frac{2}{m}})\n",
    "$$\n",
    "For m number of input units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257c18b",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15382ec2",
   "metadata": {},
   "source": [
    "To compute the cost of the output Cross-Entropy is used.\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\sum_{i=0}^{10-1}y_ilog(\\hat{y_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce850c9a",
   "metadata": {},
   "source": [
    "The parameter $y$ is a one-hot true label vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8430731",
   "metadata": {},
   "source": [
    "Though, we are interested in computing the gradient of the loss function. \\\n",
    "The derivative of $L$ is,\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta Z_1} = \\hat{y} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db595b51",
   "metadata": {},
   "source": [
    "But we want to compute the effect of any given weight on the output, and adjust thereafter. \\\n",
    "So we want the derivative of $L$ with respect to $W_0$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_0} \n",
    "$$\n",
    "Applying the chain rule to this we get,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_0} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial A_0}\\frac{\\partial A_0}{\\partial Z_0}\\frac{\\partial Z_0}{\\partial W_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72307d47",
   "metadata": {},
   "source": [
    "Which clearly shows the idea of propagating backwards, starting from the output all the way to the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3fbd6",
   "metadata": {},
   "source": [
    "This results in,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\delta_1 &= \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial Z_1} = \\hat{y} - y \\in\\mathbb{R^{10\\times 1}} \\\\[10pt]\n",
    "\\delta_0 &= \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial A_0}\\frac{\\partial A_0}{\\partial Z_0}= (W_1^T\\partial_1)\\circ\\text{ReLU}'(Z_0) \\in\\mathbb{R^{100\\times 1}} \\\\[10pt]\n",
    "\\frac{\\partial L}{\\partial W_0} &= \\partial_0 X^T \\in\\mathbb{R^{100\\times 784}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8651f5",
   "metadata": {},
   "source": [
    "With the derivative of ReLU defined as,\n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } x > 0 \\\\\n",
    "0, & \\text{if } x \\le 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bee99",
   "metadata": {},
   "source": [
    "Finally, the weights can be updated\n",
    "$$\n",
    "W_0 \\leftarrow W_0-\\eta(\\delta_0X^T) \\\\[10pt]\n",
    "W_1 \\leftarrow W_1-\\eta(\\delta_1 A_0^T)\n",
    "$$\n",
    "where $\\eta$ is the learning rate defiend as,\n",
    "$$\n",
    "\\eta(\\text{epoch}) = \n",
    "\\begin{cases}\n",
    "0.01, & \\text{epoch} < 5 \\\\\n",
    "0.01 \\cdot 0.95^{\\text{epoch}}, & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
